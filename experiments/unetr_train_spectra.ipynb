{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/data2/eranario/scratch/rgb-to-multispectral-unet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset import PotatoDatasetSpectra\n",
    "from src.model import UNeTransformedSpectral\n",
    "# from src.util import show_predictions\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f04b9a834f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set torch random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dir = \"/data2/eranario/data/Multispectral-Potato/Dataset/RGB_Images\"\n",
    "spectral_dir = \"/data2/eranario/data/Multispectral-Potato/Dataset/Spectral_Images\"\n",
    "spectral_file = \"/data2/eranario/data/Multispectral-Potato/Signals/spectra.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train data: 100%|██████████| 240/240 [00:03<00:00, 69.86it/s]\n",
      "Loading val data: 100%|██████████| 60/60 [00:00<00:00, 188.99it/s]\n",
      "Loading test data: 100%|██████████| 60/60 [00:00<00:00, 208.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 240\n",
      "Validation dataset size: 60\n",
      "Test dataset size: 60\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='train', align=False)\n",
    "val_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='val', align=False)\n",
    "test_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='test', align=False)\n",
    "\n",
    "# print the size of the datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "num_workers = 1\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGB shape: torch.Size([4, 3, 224, 224])\n",
      "Signal shape: torch.Size([4, 1, 2151])\n"
     ]
    }
   ],
   "source": [
    "# check first batch\n",
    "for rgb, signal, _, _, _, _ in train_dataloader:\n",
    "    print(f\"RGB shape: {rgb.shape}\")\n",
    "    print(f\"Signal shape: {signal.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.num_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted num_tokens to 239 to make input_dim divisible.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNeTransformedSpectral(\n",
    "    in_channels=3, \n",
    "    out_channels=len(train_dataset.channels),\n",
    "    num_bands=train_dataset.num_bands,\n",
    "    spectral_dim=512,\n",
    "    num_tokens=77\n",
    ").to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Training:   0%|          | 0/60 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 37.52 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.03 GiB is free. Process 815 has 1.50 GiB memory in use. Including non-PyTorch memory, this process has 20.22 GiB memory in use. Of the allocated memory 19.13 GiB is allocated by PyTorch, and 77.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 40\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspectral_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, spectral_images)\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/eranario/scratch/rgb-to-multispectral-unet/src/model.py:449\u001b[0m, in \u001b[0;36mUNeTransformedSpectral.forward\u001b[0;34m(self, x, spectral_signal)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# decoder\u001b[39;00m\n\u001b[1;32m    448\u001b[0m down_1, p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_conv_1(x)\n\u001b[0;32m--> 449\u001b[0m trans_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrans_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdown_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspectral_signal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m down_2, p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_conv_2(p1)\n\u001b[1;32m    451\u001b[0m trans_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans_2(down_2, spectral_signal)\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/data2/eranario/scratch/rgb-to-multispectral-unet/src/model.py:102\u001b[0m, in \u001b[0;36mTransformerBlockSpectral.forward\u001b[0;34m(self, x, signal)\u001b[0m\n\u001b[1;32m    100\u001b[0m b, c, h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    101\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b, c, h \u001b[38;5;241m*\u001b[39m w)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_output)\n\u001b[1;32m    104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/functional.py:6242\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6241\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m-> 6242\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m   6244\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m dropout(attn_output_weights, p\u001b[38;5;241m=\u001b[39mdropout_p)\n",
      "File \u001b[0;32m~/miniconda3/envs/unet/lib/python3.10/site-packages/torch/nn/functional.py:2140\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2140\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 37.52 GiB. GPU 0 has a total capacity of 23.65 GiB of which 1.03 GiB is free. Process 815 has 1.50 GiB memory in use. Including non-PyTorch memory, this process has 20.22 GiB memory in use. Of the allocated memory 19.13 GiB is allocated by PyTorch, and 77.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def evaluateEuclideanDistance(predictedImage, groundTruthImage):\n",
    "    # Compute Euclidean distance between pixels\n",
    "    pixelDifferences = np.sqrt(np.sum((predictedImage - groundTruthImage) ** 2, axis=-1))\n",
    "    # Compute the average Euclidean distance for the image\n",
    "    averagePixelDifferences = np.mean(pixelDifferences)\n",
    "    return averagePixelDifferences\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_similarities = []  # SSIM for training\n",
    "val_similarities = []    # SSIM for validation\n",
    "train_euclidean_distances = []  # Euclidean distances for training\n",
    "val_euclidean_distances = []    # Euclidean distances for validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_similarity_score = 0.0\n",
    "    train_euclidean_distance_score = 0.0\n",
    "    train_loop = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", leave=True)\n",
    "\n",
    "    for batch in train_loop:\n",
    "        rgb_images, spectral_signal, *spectral_images = batch\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        spectral_signal = spectral_signal.to(device)\n",
    "        spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(rgb_images, spectral_signal)\n",
    "        loss = criterion(outputs, spectral_images)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute SSIM and Euclidean distance for training\n",
    "        for i in range(outputs.size(0)):  # Iterate over the batch size\n",
    "            output_img = outputs[i].cpu().detach().numpy()  # Predicted image\n",
    "            groundtruth_img = spectral_images[i].cpu().detach().numpy()  # Ground truth image\n",
    "\n",
    "            # Normalize images to [0, 1] for SSIM computation\n",
    "            output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "            groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "            # Compute SSIM for each spectral band separately\n",
    "            band_ssim = [\n",
    "                ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            train_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "            # Compute Euclidean distance for each spectral band\n",
    "            band_euclidean_distance = [\n",
    "                evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            train_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_train_similarity = train_similarity_score / len(train_dataloader.dataset)\n",
    "    avg_train_euclidean_distance = train_euclidean_distance_score / len(train_dataloader.dataset)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_similarities.append(avg_train_similarity)\n",
    "    train_euclidean_distances.append(avg_train_euclidean_distance)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_similarity_score = 0.0\n",
    "    val_euclidean_distance_score = 0.0\n",
    "    val_loop = tqdm(val_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            rgb_images, *spectral_images = batch\n",
    "            rgb_images = rgb_images.to(device)\n",
    "            spectral_signal = spectral_signal.to(device)\n",
    "            spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)\n",
    "\n",
    "            outputs = model(rgb_images, spectral_signal)\n",
    "            loss = criterion(outputs, spectral_images)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute SSIM and Euclidean distance for validation\n",
    "            for i in range(outputs.size(0)):\n",
    "                output_img = outputs[i].cpu().numpy()\n",
    "                groundtruth_img = spectral_images[i].cpu().numpy()\n",
    "\n",
    "                # Normalize images to [0, 1]\n",
    "                output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "                groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "                # Compute SSIM for each spectral band separately\n",
    "                band_ssim = [\n",
    "                    ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                    for band in range(output_img.shape[0])\n",
    "                ]\n",
    "                val_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "                # Compute Euclidean distance for each spectral band\n",
    "                band_euclidean_distance = [\n",
    "                    evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                    for band in range(output_img.shape[0])\n",
    "                ]\n",
    "                val_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    avg_val_similarity = val_similarity_score / len(val_dataloader.dataset)\n",
    "    avg_val_euclidean_distance = val_euclidean_distance_score / len(val_dataloader.dataset)\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_similarities.append(avg_val_similarity)\n",
    "    val_euclidean_distances.append(avg_val_euclidean_distance)\n",
    "\n",
    "    # Log results\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Train SSIM: {avg_train_similarity:.4f}, Val SSIM: {avg_val_similarity:.4f}\")\n",
    "    print(f\"  Train Euclidean: {avg_train_euclidean_distance:.4f}, Val Euclidean: {avg_val_euclidean_distance:.4f}\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, marker='s', linestyle='--', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plotting SSIM and Euclidean Distance for Training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_similarities, marker='o', linestyle='-', label=\"Train SSIM\")\n",
    "plt.plot(range(1, num_epochs + 1), train_euclidean_distances, marker='^', linestyle='--', label=\"Train Euclidean Distance\")\n",
    "plt.title(\"Training SSIM and Euclidean Distance per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Metric Value\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plotting SSIM and Euclidean Distance for Validation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), val_similarities, marker='o', linestyle='-', label=\"Validation SSIM\")\n",
    "plt.plot(range(1, num_epochs + 1), val_euclidean_distances, marker='^', linestyle='--', label=\"Validation Euclidean Distance\")\n",
    "plt.title(\"Validation SSIM and Euclidean Distance per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Metric Value\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting training and validation loss curves with zoomed y-axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, marker='s', linestyle='--', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Adjust y-axis limits to zoom in\n",
    "min_loss = min(min(train_losses), min(val_losses))\n",
    "plt.ylim(0, min_loss * 1.5)  # Set upper limit slightly above the minimum loss for better visibility\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_similarity_score = 0.0\n",
    "test_euclidean_distance_score = 0.0\n",
    "\n",
    "test_loop = tqdm(test_dataloader, desc=\"Testing\", leave=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loop:\n",
    "        rgb_images, *spectral_images = batch\n",
    "        rgb_images = rgb_images.to(device)  # Input images\n",
    "        spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)  # Target spectral channels\n",
    "\n",
    "        outputs = model(rgb_images)\n",
    "        loss = criterion(outputs, spectral_images)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Compute SSIM and Euclidean distance for the test set\n",
    "        for i in range(outputs.size(0)):  # Iterate over the batch size\n",
    "            output_img = outputs[i].cpu().numpy()  # Predicted image\n",
    "            groundtruth_img = spectral_images[i].cpu().numpy()  # Ground truth image\n",
    "\n",
    "            # Normalize images to [0, 1] for SSIM computation\n",
    "            output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "            groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "            # Compute SSIM for each spectral band separately\n",
    "            band_ssim = [\n",
    "                ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            test_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "            # Compute Euclidean distance for each spectral band\n",
    "            band_euclidean_distance = [\n",
    "                evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            test_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "avg_test_similarity = test_similarity_score / len(test_dataloader.dataset)\n",
    "avg_test_euclidean_distance = test_euclidean_distance_score / len(test_dataloader.dataset)\n",
    "\n",
    "# Log test results\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test SSIM: {avg_test_similarity:.4f}\")\n",
    "print(f\"  Test Euclidean Distance: {avg_test_euclidean_distance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataloader, model, device, channels=None):\n",
    "    \"\"\"\n",
    "    Displays the RGB input, ground truth spectral channels, and model predictions for a single sample in a vertical layout.\n",
    "    Args:\n",
    "        dataloader: DataLoader to fetch data.\n",
    "        model: Trained model to generate predictions.\n",
    "        device: Device (CPU/GPU) to use.\n",
    "        channels: List of channel names (e.g., ['Green', 'NIR', 'Red', 'Red Edge']).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    channels = channels or ['Green', 'NIR', 'Red', 'Red Edge']  # Default channel names\n",
    "    num_spectral_channels = len(channels)\n",
    "\n",
    "    # Get one batch of data\n",
    "    rgb_images, *spectral_images = next(iter(dataloader))\n",
    "    rgb_images = rgb_images.to(device)  # Move RGB inputs to the device\n",
    "    spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)  # Ground truth\n",
    "    predictions = model(rgb_images)  # Model predictions\n",
    "\n",
    "    # Use only the first sample in the batch\n",
    "    rgb_image = rgb_images[1].permute(1, 2, 0).cpu().numpy()  # Convert to HxWxC for RGB\n",
    "    ground_truth = spectral_images[1].cpu().numpy()  # (num_channels, H, W)\n",
    "    prediction = predictions[1].detach().cpu().numpy()  # Detach, then convert to NumPy (num_channels, H, W)\n",
    "\n",
    "    # Create a vertical layout figure\n",
    "    fig, axs = plt.subplots(num_spectral_channels, 3, figsize=(15, 5 * num_spectral_channels))\n",
    "    for channel_idx in range(num_spectral_channels):\n",
    "        spectral_channel_gt = ground_truth[channel_idx]  # Ground truth for this channel\n",
    "        spectral_channel_pred = prediction[channel_idx]  # Prediction for this channel\n",
    "\n",
    "        # RGB input\n",
    "        if channel_idx == 0:  # Show RGB only in the first row\n",
    "            axs[channel_idx, 0].imshow(rgb_image)\n",
    "            axs[channel_idx, 0].set_title(\"RGB Input\")\n",
    "        else:\n",
    "            axs[channel_idx, 0].axis(\"off\")  # Keep empty for other rows\n",
    "\n",
    "        # Ground truth\n",
    "        axs[channel_idx, 1].imshow(spectral_channel_gt, cmap=\"viridis\")\n",
    "        axs[channel_idx, 1].set_title(f\"GT: {channels[channel_idx]}\")\n",
    "\n",
    "        # Prediction\n",
    "        axs[channel_idx, 2].imshow(spectral_channel_pred, cmap=\"viridis\")\n",
    "        axs[channel_idx, 2].set_title(f\"Pred: {channels[channel_idx]}\")\n",
    "\n",
    "        # Remove axes for cleaner visualization\n",
    "        axs[channel_idx, 0].axis(\"off\")\n",
    "        axs[channel_idx, 1].axis(\"off\")\n",
    "        axs[channel_idx, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['Green', 'Near Infrared', 'Red', 'Red Edge']\n",
    "show_predictions(test_dataloader, model, device, channels=channels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
