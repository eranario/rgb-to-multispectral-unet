{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"/data2/eranario/scratch/rgb-to-multispectral-unet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset import PotatoDatasetSpectra\n",
    "from src.model import UNeTransformedSpectral\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set torch random seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_dir = \"/data2/eranario/data/Multispectral-Potato/Dataset/RGB_Images\"\n",
    "spectral_dir = \"/data2/eranario/data/Multispectral-Potato/Dataset/Spectral_Images\"\n",
    "spectral_file = \"/data2/eranario/data/Multispectral-Potato/Signals/spectra.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='train', align=True)\n",
    "val_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='val', align=True)\n",
    "test_dataset = PotatoDatasetSpectra(rgb_dir, spectral_dir, spectral_file, transform=transform, mode='test', align=True)\n",
    "\n",
    "# print the size of the datasets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "num_workers = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first batch\n",
    "for rgb, signal, _, _, _, _ in train_dataloader:\n",
    "    print(f\"RGB shape: {rgb.shape}\")\n",
    "    print(f\"Signal shape: {signal.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.num_bands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNeTransformedSpectral(\n",
    "    in_channels=3, \n",
    "    out_channels=len(train_dataset.channels),\n",
    "    num_bands=train_dataset.num_bands,\n",
    "    spectral_dim=256,\n",
    "    num_tokens=196,\n",
    "    patch_size=4\n",
    ").to(device)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def evaluateEuclideanDistance(predictedImage, groundTruthImage):\n",
    "    # Compute Euclidean distance between pixels\n",
    "    pixelDifferences = np.sqrt(np.sum((predictedImage - groundTruthImage) ** 2, axis=-1))\n",
    "    # Compute the average Euclidean distance for the image\n",
    "    averagePixelDifferences = np.mean(pixelDifferences)\n",
    "    return averagePixelDifferences\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_similarities = []  # SSIM for training\n",
    "val_similarities = []    # SSIM for validation\n",
    "train_euclidean_distances = []  # Euclidean distances for training\n",
    "val_euclidean_distances = []    # Euclidean distances for validation\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_similarity_score = 0.0\n",
    "    train_euclidean_distance_score = 0.0\n",
    "    train_loop = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Training\", leave=True)\n",
    "\n",
    "    for batch in train_loop:\n",
    "        rgb_images, spectral_signal, *spectral_images = batch\n",
    "        rgb_images = rgb_images.to(device)\n",
    "        spectral_signal = spectral_signal.to(device)\n",
    "        spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(rgb_images, spectral_signal)\n",
    "        loss = criterion(outputs, spectral_images)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Compute SSIM and Euclidean distance for training\n",
    "        for i in range(outputs.size(0)):  # Iterate over the batch size\n",
    "            output_img = outputs[i].cpu().detach().numpy()  # Predicted image\n",
    "            groundtruth_img = spectral_images[i].cpu().detach().numpy()  # Ground truth image\n",
    "\n",
    "            # Normalize images to [0, 1] for SSIM computation\n",
    "            output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "            groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "            # Compute SSIM for each spectral band separately\n",
    "            band_ssim = [\n",
    "                ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            train_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "            # Compute Euclidean distance for each spectral band\n",
    "            band_euclidean_distance = [\n",
    "                evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            train_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    avg_train_similarity = train_similarity_score / len(train_dataloader.dataset)\n",
    "    avg_train_euclidean_distance = train_euclidean_distance_score / len(train_dataloader.dataset)\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_similarities.append(avg_train_similarity)\n",
    "    train_euclidean_distances.append(avg_train_euclidean_distance)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_similarity_score = 0.0\n",
    "    val_euclidean_distance_score = 0.0\n",
    "    val_loop = tqdm(val_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - Validation\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loop:\n",
    "            rgb_images, spectral_signal, *spectral_images = batch\n",
    "            rgb_images = rgb_images.to(device)\n",
    "            spectral_signal = spectral_signal.to(device)\n",
    "            spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)\n",
    "\n",
    "            outputs = model(rgb_images, spectral_signal)\n",
    "            loss = criterion(outputs, spectral_images)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Compute SSIM and Euclidean distance for validation\n",
    "            for i in range(outputs.size(0)):\n",
    "                output_img = outputs[i].cpu().numpy()\n",
    "                groundtruth_img = spectral_images[i].cpu().numpy()\n",
    "\n",
    "                # Normalize images to [0, 1]\n",
    "                output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "                groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "                # Compute SSIM for each spectral band separately\n",
    "                band_ssim = [\n",
    "                    ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                    for band in range(output_img.shape[0])\n",
    "                ]\n",
    "                val_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "                # Compute Euclidean distance for each spectral band\n",
    "                band_euclidean_distance = [\n",
    "                    evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                    for band in range(output_img.shape[0])\n",
    "                ]\n",
    "                val_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    avg_val_similarity = val_similarity_score / len(val_dataloader.dataset)\n",
    "    avg_val_euclidean_distance = val_euclidean_distance_score / len(val_dataloader.dataset)\n",
    "\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_similarities.append(avg_val_similarity)\n",
    "    val_euclidean_distances.append(avg_val_euclidean_distance)\n",
    "\n",
    "    # Log results\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"  Train SSIM: {avg_train_similarity:.4f}, Val SSIM: {avg_val_similarity:.4f}\")\n",
    "    print(f\"  Train Euclidean: {avg_train_euclidean_distance:.4f}, Val Euclidean: {avg_val_euclidean_distance:.4f}\")\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training and validation loss curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, marker='s', linestyle='--', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plotting SSIM and Euclidean Distance for Training\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_similarities, marker='o', linestyle='-', label=\"Train SSIM\")\n",
    "plt.plot(range(1, num_epochs + 1), train_euclidean_distances, marker='^', linestyle='--', label=\"Train Euclidean Distance\")\n",
    "plt.title(\"Training SSIM and Euclidean Distance per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Metric Value\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "# Plotting SSIM and Euclidean Distance for Validation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), val_similarities, marker='o', linestyle='-', label=\"Validation SSIM\")\n",
    "plt.plot(range(1, num_epochs + 1), val_euclidean_distances, marker='^', linestyle='--', label=\"Validation Euclidean Distance\")\n",
    "plt.title(\"Validation SSIM and Euclidean Distance per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Metric Value\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting training and validation loss curves with zoomed y-axis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, marker='o', linestyle='-', label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, marker='s', linestyle='--', label=\"Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss per Epoch\", fontsize=16)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Adjust y-axis limits to zoom in\n",
    "min_loss = min(min(train_losses), min(val_losses))\n",
    "plt.ylim(0, min_loss * 1.5)  # Set upper limit slightly above the minimum loss for better visibility\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phase\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "test_similarity_score = 0.0\n",
    "test_euclidean_distance_score = 0.0\n",
    "\n",
    "test_loop = tqdm(test_dataloader, desc=\"Testing\", leave=True)\n",
    "\n",
    "# Initialize variables to store data for visualization\n",
    "stored_rgb_images = None\n",
    "stored_self_attn_weights = None\n",
    "stored_cross_attn_weights = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loop:\n",
    "        rgb_images, spectral_signal, *spectral_images = batch\n",
    "        rgb_images = rgb_images.to(device)  # Input images\n",
    "        spectral_signal = spectral_signal.to(device)  # Spectral signal\n",
    "        spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)  # Target spectral channels\n",
    "\n",
    "        outputs, attn_weights = model(rgb_images, spectral_signal, get_weights=True)\n",
    "        loss = criterion(outputs, spectral_images)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store data for visualization (only from the first batch)\n",
    "        if stored_rgb_images is None and stored_self_attn_weights is None and stored_cross_attn_weights is None:\n",
    "            stored_rgb_images = rgb_images.cpu()  # Store as CPU tensor\n",
    "            stored_self_attn_weights = attn_weights[\"self\"]  # Store self-attention weights\n",
    "            stored_cross_attn_weights = attn_weights[\"cross\"]  # Store cross-attention weights\n",
    "\n",
    "        # Compute SSIM and Euclidean distance for the test set\n",
    "        for i in range(outputs.size(0)):  # Iterate over the batch size\n",
    "            output_img = outputs[i].cpu().numpy()  # Predicted image\n",
    "            groundtruth_img = spectral_images[i].cpu().numpy()  # Ground truth image\n",
    "\n",
    "            # Normalize images to [0, 1] for SSIM computation\n",
    "            output_img = (output_img - output_img.min()) / (output_img.max() - output_img.min() + 1e-8)\n",
    "            groundtruth_img = (groundtruth_img - groundtruth_img.min()) / (groundtruth_img.max() - groundtruth_img.min() + 1e-8)\n",
    "\n",
    "            # Compute SSIM for each spectral band separately\n",
    "            band_ssim = [\n",
    "                ssim(output_img[band], groundtruth_img[band], data_range=1.0)\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            test_similarity_score += np.mean(band_ssim)\n",
    "\n",
    "            # Compute Euclidean distance for each spectral band\n",
    "            band_euclidean_distance = [\n",
    "                evaluateEuclideanDistance(output_img[band], groundtruth_img[band])\n",
    "                for band in range(output_img.shape[0])\n",
    "            ]\n",
    "            test_euclidean_distance_score += np.mean(band_euclidean_distance)\n",
    "\n",
    "avg_test_loss = test_loss / len(test_dataloader)\n",
    "avg_test_similarity = test_similarity_score / len(test_dataloader.dataset)\n",
    "avg_test_euclidean_distance = test_euclidean_distance_score / len(test_dataloader.dataset)\n",
    "\n",
    "# Log test results\n",
    "print(\"\\nTest Results:\")\n",
    "print(f\"  Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"  Test SSIM: {avg_test_similarity:.4f}\")\n",
    "print(f\"  Test Euclidean Distance: {avg_test_euclidean_distance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataloader, model, device, channels=None):\n",
    "    \"\"\"\n",
    "    Displays the RGB input, ground truth spectral channels, and model predictions for a single sample in a vertical layout.\n",
    "    Args:\n",
    "        dataloader: DataLoader to fetch data.\n",
    "        model: Trained model to generate predictions.\n",
    "        device: Device (CPU/GPU) to use.\n",
    "        channels: List of channel names (e.g., ['Green', 'NIR', 'Red', 'Red Edge']).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    channels = channels or ['Green', 'NIR', 'Red', 'Red Edge']  # Default channel names\n",
    "    num_spectral_channels = len(channels)\n",
    "\n",
    "    # Get one batch of data\n",
    "    rgb_images, spectral_signal, *spectral_images = next(iter(dataloader))\n",
    "    rgb_images = rgb_images.to(device)  # Move RGB inputs to the device\n",
    "    spectral_signal = spectral_signal.to(device)  # Move spectral signal to the device\n",
    "    spectral_images = torch.stack(spectral_images, dim=1).squeeze(2).to(device)  # Ground truth\n",
    "    predictions = model(rgb_images, spectral_signal)  # Model predictions\n",
    "\n",
    "    # Use only the first sample in the batch\n",
    "    rgb_image = rgb_images[1].permute(1, 2, 0).cpu().numpy()  # Convert to HxWxC for RGB\n",
    "    ground_truth = spectral_images[1].cpu().numpy()  # (num_channels, H, W)\n",
    "    prediction = predictions[1].detach().cpu().numpy()  # Detach, then convert to NumPy (num_channels, H, W)\n",
    "\n",
    "    # Create a vertical layout figure\n",
    "    fig, axs = plt.subplots(num_spectral_channels, 3, figsize=(15, 5 * num_spectral_channels))\n",
    "    for channel_idx in range(num_spectral_channels):\n",
    "        spectral_channel_gt = ground_truth[channel_idx]  # Ground truth for this channel\n",
    "        spectral_channel_pred = prediction[channel_idx]  # Prediction for this channel\n",
    "\n",
    "        # RGB input\n",
    "        if channel_idx == 0:  # Show RGB only in the first row\n",
    "            axs[channel_idx, 0].imshow(rgb_image)\n",
    "            axs[channel_idx, 0].set_title(\"RGB Input\")\n",
    "        else:\n",
    "            axs[channel_idx, 0].axis(\"off\")  # Keep empty for other rows\n",
    "\n",
    "        # Ground truth\n",
    "        axs[channel_idx, 1].imshow(spectral_channel_gt, cmap=\"viridis\")\n",
    "        axs[channel_idx, 1].set_title(f\"GT: {channels[channel_idx]}\")\n",
    "\n",
    "        # Prediction\n",
    "        axs[channel_idx, 2].imshow(spectral_channel_pred, cmap=\"viridis\")\n",
    "        axs[channel_idx, 2].set_title(f\"Pred: {channels[channel_idx]}\")\n",
    "\n",
    "        # Remove axes for cleaner visualization\n",
    "        axs[channel_idx, 0].axis(\"off\")\n",
    "        axs[channel_idx, 1].axis(\"off\")\n",
    "        axs[channel_idx, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = ['Green', 'Near Infrared', 'Red', 'Red Edge']\n",
    "show_predictions(test_dataloader, model, device, channels=channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def visualize_self_attention_with_image(attention_weights, num_layers, input_image, layer_names=None):\n",
    "    \"\"\"\n",
    "    Visualize self-attention weights averaged across heads for each Transformer layer alongside the input image.\n",
    "\n",
    "    Args:\n",
    "        attention_weights (list[torch.Tensor]): List of attention weights for each layer.\n",
    "            Each tensor should have shape (num_heads, num_patches, num_patches).\n",
    "        num_layers (int): Number of layers to visualize.\n",
    "        input_image (torch.Tensor or np.ndarray): Input image with shape (H, W, C) or (H, W).\n",
    "        layer_names (list[str], optional): Names of the layers for display.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6 * num_layers))\n",
    "\n",
    "    # Normalize the image for display\n",
    "    if isinstance(input_image, torch.Tensor):\n",
    "        input_image = input_image.detach().cpu().numpy()\n",
    "    if input_image.shape[-1] == 3:  # If RGB image\n",
    "        input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "\n",
    "    for i, attn in enumerate(attention_weights[:num_layers]):\n",
    "        # Average over heads\n",
    "        avg_attn = attn.mean(dim=0).detach().cpu()  # Shape: (num_patches, num_patches)\n",
    "\n",
    "        # Determine the patch grid shape dynamically\n",
    "        num_patches = avg_attn.shape[0]\n",
    "        grid_size = int(math.sqrt(num_patches))\n",
    "        if grid_size * grid_size != num_patches:\n",
    "            raise ValueError(f\"Number of patches ({num_patches}) is not a perfect square.\")\n",
    "\n",
    "        # Reshape the attention map\n",
    "        avg_attn = avg_attn.mean(dim=1)  # Average over all patches\n",
    "        avg_attn = avg_attn.view(grid_size, grid_size)\n",
    "\n",
    "        # Resize attention map to match image dimensions\n",
    "        resized_attn = torch.nn.functional.interpolate(\n",
    "            avg_attn.unsqueeze(0).unsqueeze(0),  # Add batch and channel dims\n",
    "            size=input_image.shape[:2],  # Match input image size\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).squeeze().numpy()  # Remove extra dims\n",
    "\n",
    "        # Plot the input image and the attention map\n",
    "        plt.subplot(num_layers, 2, 2 * i + 1)\n",
    "        plt.imshow(input_image, cmap=\"gray\" if input_image.ndim == 2 else None)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Input Image\")\n",
    "\n",
    "        plt.subplot(num_layers, 2, 2 * i + 2)\n",
    "        plt.imshow(input_image, cmap=\"gray\" if input_image.ndim == 2 else None)\n",
    "        plt.imshow(resized_attn, cmap=\"viridis\", alpha=0.5)  # Overlay attention map\n",
    "        plt.colorbar()\n",
    "        layer_title = layer_names[i] if layer_names else f\"Transformer Block {i + 1}\"\n",
    "        plt.title(f\"Self-Attention Map - {layer_title}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention maps from stored data\n",
    "if stored_rgb_images is not None and stored_self_attn_weights is not None:\n",
    "    # Take the first image from the stored batch\n",
    "    input_image = stored_rgb_images[1].permute(1, 2, 0).numpy()  # Convert to HWC format\n",
    "\n",
    "    # Normalize the input image for visualization\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "\n",
    "    # Self-attention weights\n",
    "    self_attentions = stored_self_attn_weights\n",
    "\n",
    "    # Number of layers and optional layer names\n",
    "    num_layers = len(self_attentions)\n",
    "    layer_names = [f\"Transformer Block {i + 1}\" for i in range(num_layers)]\n",
    "\n",
    "    # Visualize self-attention weights with the input image\n",
    "    visualize_self_attention_with_image(\n",
    "        attention_weights=self_attentions,\n",
    "        num_layers=num_layers,\n",
    "        input_image=input_image,\n",
    "        layer_names=layer_names\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cross_attention_for_patch(\n",
    "    cross_attention_weights_all_blocks,\n",
    "    input_image,\n",
    "    patch_coords,\n",
    "    layer_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize cross-attention scores for a specific patch in the image across Transformer blocks.\n",
    "\n",
    "    Args:\n",
    "        cross_attention_weights_all_blocks (list[torch.Tensor]): List of cross-attention weights for each block.\n",
    "            Each tensor should have shape (num_heads, num_patches, num_keys).\n",
    "        input_image (torch.Tensor or np.ndarray): Input image in HWC format.\n",
    "        patch_coords (tuple[int, int]): Coordinates (row, col) of the selected patch in the grid.\n",
    "        layer_names (list[str], optional): Names of the Transformer blocks.\n",
    "    \"\"\"\n",
    "    num_blocks = len(cross_attention_weights_all_blocks)\n",
    "    selected_row, selected_col = patch_coords\n",
    "\n",
    "    plt.figure(figsize=(15, 6 * num_blocks))\n",
    "\n",
    "    for block_idx, cross_attention_weights in enumerate(cross_attention_weights_all_blocks):\n",
    "        # Average attention weights across heads\n",
    "        avg_cross_attention = cross_attention_weights.mean(dim=0).detach().cpu()  # Shape: (num_patches, num_keys)\n",
    "\n",
    "        # Calculate patch grid dimensions dynamically\n",
    "        num_patches = avg_cross_attention.shape[0]\n",
    "        grid_size = int(math.sqrt(num_patches))\n",
    "        if grid_size * grid_size != num_patches:\n",
    "            raise ValueError(f\"Number of patches ({num_patches}) is not a perfect square.\")\n",
    "        h_patches, w_patches = grid_size, grid_size\n",
    "\n",
    "        # Convert (row, col) to patch index\n",
    "        patch_idx = selected_row * w_patches + selected_col\n",
    "        if patch_idx >= num_patches:\n",
    "            raise IndexError(f\"Patch index {patch_idx} is out of bounds for grid size {h_patches}x{w_patches}.\")\n",
    "\n",
    "        # Extract attention scores for the selected patch\n",
    "        patch_attention_scores = avg_cross_attention[patch_idx]  # Shape: (num_keys,)\n",
    "\n",
    "        # Highlight the selected patch on the input image\n",
    "        patch_height = input_image.shape[0] // h_patches\n",
    "        patch_width = input_image.shape[1] // w_patches\n",
    "        top, left = selected_row * patch_height, selected_col * patch_width\n",
    "\n",
    "        input_image_display = input_image.copy()\n",
    "        if isinstance(input_image_display, torch.Tensor):\n",
    "            input_image_display = input_image_display.numpy()\n",
    "        input_image_display = (input_image_display - input_image_display.min()) / (\n",
    "            input_image_display.max() - input_image_display.min()\n",
    "        )  # Normalize\n",
    "\n",
    "        plt.subplot(num_blocks, 2, 2 * block_idx + 1)\n",
    "        plt.imshow(input_image_display, cmap=\"gray\" if input_image_display.ndim == 2 else None)\n",
    "        plt.gca().add_patch(plt.Rectangle(\n",
    "            (left, top), patch_width, patch_height, edgecolor='red', facecolor='none', linewidth=2))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Input Image with Patch Highlighted (Block {block_idx + 1})\")\n",
    "\n",
    "        # Visualize attention scores for the spectral signal\n",
    "        plt.subplot(num_blocks, 2, 2 * block_idx + 2)\n",
    "        plt.bar(range(len(patch_attention_scores)), patch_attention_scores, color=\"blue\", alpha=0.7)\n",
    "        plt.xlabel(\"Attention Keys\")\n",
    "        plt.ylabel(\"Attention Score\")\n",
    "        block_name = layer_names[block_idx] if layer_names else f\"Transformer Block {block_idx + 1}\"\n",
    "        plt.title(f\"Cross-Attention Scores for Selected Patch - {block_name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-attention maps for a specific patch\n",
    "if stored_rgb_images is not None and stored_cross_attn_weights is not None:\n",
    "    # Take the first image from the stored batch\n",
    "    input_image = stored_rgb_images[1].permute(1, 2, 0).numpy()  # Convert to HWC format\n",
    "\n",
    "    # Normalize the input image for visualization\n",
    "    input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "\n",
    "    # Cross-attention weights\n",
    "    cross_attentions = stored_cross_attn_weights\n",
    "\n",
    "    # Number of layers and optional layer names\n",
    "    num_layers = len(cross_attentions)\n",
    "    layer_names = [f\"Transformer Block {i + 1}\" for i in range(num_layers)]\n",
    "\n",
    "    # Select a patch (row, col) in the grid\n",
    "    selected_patch_coords = (3, 3)  # Example: Center patch\n",
    "\n",
    "    # Visualize cross-attention scores for the selected patch\n",
    "    visualize_cross_attention_for_patch(\n",
    "        cross_attention_weights_all_blocks=cross_attentions,\n",
    "        input_image=input_image,\n",
    "        patch_coords=selected_patch_coords,\n",
    "        layer_names=layer_names\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
